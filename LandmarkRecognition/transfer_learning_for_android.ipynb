{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: based  on “Recognize Flowers with TensorFlow on Android” TensorFlow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = \"top_landmarks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the dataset as images and breaks into a training and a validation batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2039 images belonging to 9 classes.\n",
      "Found 505 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 224, 224, 3), (64, 9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in train_generator:\n",
    "  break\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coin labels are saved in \"labels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bled_island': 0, 'eiffel_tower': 1, 'golden_gate_bridge': 2, 'grand_canyon': 3, 'machu_picchu': 4, 'niagara_falls': 5, 'presern_square': 6, 'stonehenge': 7, 'sydney_opera_house': 8}\n"
     ]
    }
   ],
   "source": [
    "print (train_generator.class_indices)\n",
    "\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "\n",
    "with open('labels.txt', 'w') as f:\n",
    "  f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the base model from a pre-trained network\n",
    "Create the base model from the MobileNet V2 model developed at Google, and pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\n",
    "\n",
    "First, pick which intermediate layer of MobileNet V2 will be used for feature extraction. A common practice is to use the output of the very last layer before the flatten operation, the so-called \"bottleneck layer\". The reasoning here is that the following fully-connected layers will be too specialized to the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.\n",
    "\n",
    "Let's instantiate an MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                              include_top=False, \n",
    "                                              weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "You will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(9, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 5, 32)          368672    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5, 5, 32)          0         \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 32)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 297       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2626953 (10.02 MB)\n",
      "Trainable params: 368969 (1.41 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a convolution layer and a dense layer that can be trained. Here is the list of all the variables that will be trained (note that these are matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable variables = 4\n",
      "Variable:  conv2d_2/kernel:0\n",
      "Variable:  conv2d_2/bias:0\n",
      "Variable:  dense_2/kernel:0\n",
      "Variable:  dense_2/bias:0\n"
     ]
    }
   ],
   "source": [
    "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))\n",
    "variables_names = [v.name for v in model.trainable_variables]\n",
    "for k in variables_names:\n",
    "    print(\"Variable: \", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Ideally, you would like to train your model with, say, 10 epoch. However, if this takes prohibitively long time on your laptop, for the exercise sake use only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 128s 4s/step - loss: 1.3449 - accuracy: 0.5866 - val_loss: 0.6289 - val_accuracy: 0.8376\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 94s 3s/step - loss: 0.3255 - accuracy: 0.9039 - val_loss: 0.3270 - val_accuracy: 0.9208\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 92s 3s/step - loss: 0.1052 - accuracy: 0.9745 - val_loss: 0.2901 - val_accuracy: 0.9149\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 89s 3s/step - loss: 0.0563 - accuracy: 0.9902 - val_loss: 0.2983 - val_accuracy: 0.9228\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.0331 - accuracy: 0.9961 - val_loss: 0.2763 - val_accuracy: 0.9327\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.0190 - accuracy: 0.9995 - val_loss: 0.2626 - val_accuracy: 0.9366\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 89s 3s/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9426\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 89s 3s/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9386\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 88s 3s/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2835 - val_accuracy: 0.9386\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 86s 3s/step - loss: 0.0078 - accuracy: 0.9995 - val_loss: 0.2725 - val_accuracy: 0.9366\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # ideally 10\n",
    "\n",
    "history = model.fit(train_generator, epochs=epochs, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models\\assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_dir = 'saved_models' # Put the name of the directory where you want to save the model\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "\n",
    "#converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "#tflite_model = converter.convert()\n",
    "\n",
    "#with open('model.tflite', 'wb') as f:\n",
    "#  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model from TensorFlow to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lookitecture_model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the above code does not run without errors with your version of TF and Python, please use \"tflite_convert --saved_model_dir=YOUR_MODEL_DIR --output_file=mobilenet.tflite\" from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding metadata to TensorFlow Lite models \n",
    "TensorFlow Lite metadata provides a standard for model descriptions. The metadata is an important source of knowledge about what the model does and its input / output information. The metadata consists of both\n",
    "\n",
    "human readable parts which convey the best practice when using the model, and\n",
    "machine readable parts that can be leveraged by code generators, such as the TensorFlow Lite Android code generator and the Android Studio ML Binding feature.\n",
    "\n",
    "You can find more details about TF Lite model metadata here: https://www.tensorflow.org/lite/models/convert/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_support import metadata_schema_py_generated as _metadata_fb\n",
    "from tflite_support import metadata as _metadata\n",
    "import flatbuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSpecificInfo(object):\n",
    "  \"\"\"Holds information that is specificly tied to an image classifier.\"\"\"\n",
    "\n",
    "  def __init__(self, name, version, image_width, image_height, image_min,\n",
    "               image_max, mean, std, num_classes, author):\n",
    "    self.name = name\n",
    "    self.version = version\n",
    "    self.image_width = image_width\n",
    "    self.image_height = image_height\n",
    "    self.image_min = image_min\n",
    "    self.image_max = image_max\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "    self.num_classes = num_classes\n",
    "    self.author = author\n",
    "\n",
    "_MODEL_INFO = {\n",
    "    \"lookitecture_model.tflite\":\n",
    "        ModelSpecificInfo(\n",
    "            name=\"MobileNetV2\",\n",
    "            version=\"v1\",\n",
    "            image_width=224,\n",
    "            image_height=224,\n",
    "            image_min=0,\n",
    "            image_max=255,\n",
    "            mean=[127.5],\n",
    "            std=[127.5],\n",
    "            num_classes=9,\n",
    "            author=\"TensorFlow\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataPopulatorForImageClassifier(object):\n",
    "  \"\"\"Populates the metadata for an image classifier.\"\"\"\n",
    "\n",
    "  def __init__(self, model_file, model_info, label_file_path):\n",
    "    self.model_file = model_file\n",
    "    self.model_info = model_info\n",
    "    self.label_file_path = label_file_path\n",
    "    self.metadata_buf = None\n",
    "\n",
    "  def populate(self):\n",
    "    \"\"\"Creates metadata and then populates it for an image classifier.\"\"\"\n",
    "    self._create_metadata()\n",
    "    self._populate_metadata()\n",
    "\n",
    "  def _create_metadata(self):\n",
    "    \"\"\"Creates the metadata for an image classifier.\"\"\"\n",
    "\n",
    "    # Creates model info.\n",
    "    model_meta = _metadata_fb.ModelMetadataT()\n",
    "    model_meta.name = self.model_info.name\n",
    "    model_meta.description = (\"Identify the most prominent object in the \"\n",
    "                              \"image from a set of %d categories.\" %\n",
    "                              self.model_info.num_classes)\n",
    "    model_meta.version = self.model_info.version\n",
    "    model_meta.author = self.model_info.author\n",
    "    model_meta.license = (\"Apache License. Version 2.0 \"\n",
    "                          \"http://www.apache.org/licenses/LICENSE-2.0.\")\n",
    "\n",
    "    # Creates input info.\n",
    "    input_meta = _metadata_fb.TensorMetadataT()\n",
    "    input_meta.name = \"image\"\n",
    "    input_meta.description = (\n",
    "        \"Input image to be classified. The expected image is {0} x {1}, with \"\n",
    "        \"three channels (red, blue, and green) per pixel. Each value in the \"\n",
    "        \"tensor is a single byte between {2} and {3}.\".format(\n",
    "            self.model_info.image_width, self.model_info.image_height,\n",
    "            self.model_info.image_min, self.model_info.image_max))\n",
    "    input_meta.content = _metadata_fb.ContentT()\n",
    "    input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n",
    "    input_meta.content.contentProperties.colorSpace = (\n",
    "        _metadata_fb.ColorSpaceType.RGB)\n",
    "    input_meta.content.contentPropertiesType = (\n",
    "        _metadata_fb.ContentProperties.ImageProperties)\n",
    "    input_normalization = _metadata_fb.ProcessUnitT()\n",
    "    input_normalization.optionsType = (\n",
    "        _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n",
    "    input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
    "    input_normalization.options.mean = self.model_info.mean\n",
    "    input_normalization.options.std = self.model_info.std\n",
    "    input_meta.processUnits = [input_normalization]\n",
    "    input_stats = _metadata_fb.StatsT()\n",
    "    input_stats.max = [self.model_info.image_max]\n",
    "    input_stats.min = [self.model_info.image_min]\n",
    "    input_meta.stats = input_stats\n",
    "\n",
    "    # Creates output info.\n",
    "    output_meta = _metadata_fb.TensorMetadataT()\n",
    "    output_meta.name = \"probability\"\n",
    "    output_meta.description = \"Probabilities of the %d labels respectively.\" % self.model_info.num_classes\n",
    "    output_meta.content = _metadata_fb.ContentT()\n",
    "    output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\n",
    "    output_meta.content.contentPropertiesType = (\n",
    "        _metadata_fb.ContentProperties.FeatureProperties)\n",
    "    output_stats = _metadata_fb.StatsT()\n",
    "    output_stats.max = [1.0]\n",
    "    output_stats.min = [0.0]\n",
    "    output_meta.stats = output_stats\n",
    "    label_file = _metadata_fb.AssociatedFileT()\n",
    "    label_file.name = os.path.basename(self.label_file_path)\n",
    "    label_file.description = \"Labels for objects that the model can recognize.\"\n",
    "    label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n",
    "    output_meta.associatedFiles = [label_file]\n",
    "\n",
    "    # Creates subgraph info.\n",
    "    subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "    subgraph.inputTensorMetadata = [input_meta]\n",
    "    subgraph.outputTensorMetadata = [output_meta]\n",
    "    model_meta.subgraphMetadata = [subgraph]\n",
    "\n",
    "    b = flatbuffers.Builder(0)\n",
    "    b.Finish(\n",
    "        model_meta.Pack(b),\n",
    "        _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "    self.metadata_buf = b.Output()\n",
    "\n",
    "  def _populate_metadata(self):\n",
    "    \"\"\"Populates metadata and label file to the model file.\"\"\"\n",
    "    populator = _metadata.MetadataPopulator.with_model_file(self.model_file)\n",
    "    populator.load_metadata_buffer(self.metadata_buf)\n",
    "    populator.load_associated_files([self.label_file_path])\n",
    "    populator.populate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"lookitecture_model.tflite\"\n",
    "model_basename = os.path.basename(model_file)\n",
    "\n",
    "export_model_path = os.path.join(saved_model_dir, model_basename)\n",
    "\n",
    "tf.io.gfile.copy(model_file, export_model_path, overwrite=True)\n",
    "\n",
    "populator = MetadataPopulatorForImageClassifier(\n",
    "  export_model_path, _MODEL_INFO.get(model_file), \"labels.txt\")\n",
    "populator.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "displayer = _metadata.MetadataDisplayer.with_model_file(export_model_path)\n",
    "export_json_file = os.path.join(saved_model_dir, os.path.splitext(model_basename)[0] + \".json\")\n",
    "json_file = displayer.get_metadata_json()\n",
    "with open(export_json_file, \"w\") as f:\n",
    "    f.write(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished populating metadata and associated file to the model:\n",
      "lookitecture_model.tflite\n",
      "The metadata json file has been saved to:\n",
      "saved_models\\lookitecture_model.json\n",
      "The associated file that has been been packed to the model is:\n",
      "['labels.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\"Finished populating metadata and associated file to the model:\")\n",
    "print(model_file)\n",
    "print(\"The metadata json file has been saved to:\")\n",
    "print(export_json_file)\n",
    "print(\"The associated file that has been been packed to the model is:\")\n",
    "print(displayer.get_packed_associated_file_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done creating the model! The model file you want to use in your Android app is stored in the \"saved_models\" folder. Go back to the lab instructions to see how to load it in your new app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobileenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
